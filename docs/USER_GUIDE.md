# üìñ Guide d'Utilisation D√©taill√© - Need Scanner

## Table des Mati√®res

1. [Premiers Pas](#premiers-pas)
2. [Collection de Donn√©es](#collection-de-donn√©es)
3. [Analyse et Pipeline](#analyse-et-pipeline)
4. [Interpr√©tation des R√©sultats](#interpr√©tation-des-r√©sultats)
5. [Configuration Avanc√©e](#configuration-avanc√©e)
6. [Cas d'Usage](#cas-dusage)
7. [Troubleshooting](#troubleshooting)

---

## Premiers Pas

### Installation

```bash
# 1. Cloner et installer
git clone <repo-url>
cd need_scanner
python -m venv env
source env/bin/activate
pip install -r requirements.txt

# 2. Configuration
cp .env.example .env
# √âditer .env et ajouter OPENAI_API_KEY=sk-...
```

### V√©rification

```bash
# Tester les commandes disponibles
python -m need_scanner --help

# Lister toutes les commandes
python -m need_scanner collect-reddit-multi --help
```

---

## Collection de Donn√©es

### Reddit Multi-Subreddits (Recommand√©)

**Collection rapide** :
```bash
python -m need_scanner collect-reddit-multi --limit-per-sub 30
```

**Options** :
- `--config-file` : Fichier de config des subreddits (d√©faut: `config/reddit_subs.txt`)
- `--limit-per-sub` : Posts par subreddit (d√©faut: 30)
- `--sleep-between` : D√©lai entre subreddits en secondes (d√©faut: 2.0)
- `--output` : R√©pertoire de sortie (d√©faut: `data/raw`)

**Personnaliser les subreddits** :

√âditer `config/reddit_subs.txt` :
```
# Votre secteur
YourNiche
YourIndustry

# Business g√©n√©ral
freelance
Entrepreneur
```

### Hacker News

```bash
# Derniers 30 jours, min 20 points
python -m need_scanner collect-hn --days 30 --min-points 20 --limit 100
```

**Requ√™tes personnalis√©es** :
```bash
python -m need_scanner collect-hn --queries "Ask HN,Show HN,Tell HN"
```

### Stack Exchange

```bash
# Sites workplace + freelancing
python -m need_scanner collect-stackexchange \
  --sites workplace,freelancing \
  --days 7 \
  --min-score 10 \
  --limit 50
```

**Sites disponibles** :
- `stackoverflow` - Programmation g√©n√©rale
- `workplace` - Questions professionnelles
- `freelancing` - Questions freelance
- `startups` - Questions startup
- `softwareengineering` - Architecture logicielle
- Voir `config/stackexchange_sites.txt` pour la liste compl√®te

### RSS Feeds

```bash
python -m need_scanner collect-rss \
  --feeds-file config/rss_feeds.txt \
  --days 30
```

√âditer `config/rss_feeds.txt` :
```
https://example.com/feed.xml
https://another-blog.com/rss
```

### Product Hunt

**Pr√©requis** : Token API

1. Obtenir token : https://www.producthunt.com/v2/oauth/applications
2. Ajouter √† `.env` : `PRODUCTHUNT_API_TOKEN=your_token`
3. Collecter :

```bash
python -m need_scanner collect-producthunt \
  --days 7 \
  --limit 100 \
  --categories "developer-tools,saas,productivity"
```

### Collection Multi-Sources

```bash
python -m need_scanner collect-all \
  --reddit-subreddit freelance \
  --reddit-limit 200 \
  --hn-days 30 \
  --rss-feeds-file config/rss_feeds.txt \
  --filter-lang en \
  --filter-intent \
  --min-score 5
```

**Filtres disponibles** :
- `--filter-lang` : Langues √† garder (ex: `en,fr`)
- `--filter-intent` : Garder seulement pain/request
- `--min-score` : Score minimum
- `--min-comments` : Commentaires minimum

---

## Analyse et Pipeline

### 1. Pr√©visualisation (Recommand√©)

Avant de lancer le pipeline complet, pr√©visualisez vos donn√©es :

```bash
python -m need_scanner prefilter \
  --input-pattern "data/raw/posts_*.json" \
  --filter-lang en \
  --detect-wtp \
  --show-sample 10
```

**Sortie** :
- Distribution des sources
- Distribution des langues (23+ d√©tect√©es)
- Distribution des intents (pain, request, howto, etc.)
- Signaux WTP d√©tect√©s
- √âchantillon de posts

**Interpr√©tation** :
- Si < 10 posts apr√®s filtres ‚Üí Ajuster les filtres ou collecter plus
- V√©rifier que les intents "pain/request/howto" sont suffisants
- Noter le % de posts avec WTP signals

### 2. Pipeline Complet

```bash
python -m need_scanner run \
  --input "data/raw/posts_*.json" \
  --clusters 5 \
  --output-dir data/results
```

**Options** :
- `--input` : Pattern de fichiers d'entr√©e
- `--clusters` : Nombre de clusters (d√©faut: 5, automatique si non sp√©cifi√©)
- `--model-sum` : Mod√®le LLM (d√©faut: gpt-4o-mini)
- `--max-examples` : Exemples par cluster pour LLM (d√©faut: 5)
- `--output-dir` : R√©pertoire de sortie

**√âtapes du Pipeline** :
1. Chargement des posts
2. Nettoyage et normalisation
3. D√©duplication (4 strat√©gies)
4. G√©n√©ration d'embeddings
5. Clustering (KMeans)
6. Summarization enrichie (LLM)
7. Priority scoring
8. Export (JSON + possibilit√© CSV)

### 3. Estimation des Co√ªts

Avant de lancer le pipeline complet :

```bash
python -m need_scanner estimate \
  --input "data/raw/posts_*.json" \
  --clusters 5
```

**Sortie** :
```
Estimated costs:
  Embeddings (764 tokens): $0.00001
  Summaries (5 clusters): $0.0010
  Total: $0.0011
```

---

## Interpr√©tation des R√©sultats

### Structure des Fichiers de Sortie

```
data/results/
‚îú‚îÄ‚îÄ cluster_results.json    # R√©sultats complets
‚îú‚îÄ‚îÄ embeddings.npy          # Vecteurs d'embeddings
‚îî‚îÄ‚îÄ meta.json              # M√©tadonn√©es des posts
```

### Format JSON

```json
{
  "statistics": {
    "total_posts": 200,
    "after_dedup": 43,
    "num_clusters": 5,
    "total_cost_usd": 0.0012
  },
  "insights": [
    {
      "cluster_id": 0,
      "rank": 1,
      "priority_score": 5.68,
      "summary": {
        "title": "Mauvaise pr√©paration projet",
        "problem": "Description du probl√®me...",
        "persona": "Entrepreneur en technologie",
        "jtbd": "Quand..., je veux..., afin de...",
        "context": "Contexte d'usage...",
        "monetizable": true,
        "mvp": "Proposition de MVP...",
        "alternatives": ["tool1", "tool2"],
        "willingness_to_pay_signal": "Signal d√©tect√©...",
        "pain_score_llm": 9
      },
      "pain_score_final": 8,
      "heuristic_score": 7.5,
      "traction_score": 5.0,
      "novelty_score": 10.0,
      "source_mix": ["reddit", "hn"],
      "examples": [...]
    }
  ]
}
```

### Scores Expliqu√©s

**Priority Score (0-10)** :
- Formule : `30% Pain + 25% Traction + 20% Novelty + 15% WTP + 10% Recency`
- >7 = Tr√®s haute priorit√©
- 5-7 = Haute priorit√©
- 3-5 = Priorit√© moyenne
- <3 = Faible priorit√©

**Pain Score LLM (1-10)** :
- √âvalu√© par GPT-4o-mini
- 10 = Douleur forte, urgente, r√©currente
- 5 = Douleur mod√©r√©e
- 1 = Douleur mineure

**Traction Score (0-10)** :
- Bas√© sur engagement (votes, commentaires)
- Indicateur de validation du probl√®me

**Novelty Score (0-10)** :
- Bas√© sur nombre d'alternatives
- 10 = Aucune alternative (espace vide!)
- 0 = March√© satur√©

**WTP Score (0-10)** :
- Signaux de volont√© de payer d√©tect√©s
- 10 = Multiples signaux forts
- 0 = Aucun signal

### Export CSV

Pour cr√©er un CSV enrichi (20 colonnes) :

```python
# Utiliser le script fourni ou cr√©er le v√¥tre
python -c "
import json
from pathlib import Path
from need_scanner.schemas import EnrichedInsight, EnrichedClusterSummary
from need_scanner.export.writer import write_enriched_insights_csv

# Charger r√©sultats
with open('data/results/cluster_results.json') as f:
    data = json.load(f)

# Convertir et exporter
# (voir test_sprint1.py pour exemple complet)
"
```

**Colonnes CSV** :
1. rank
2. cluster_id
3. size
4. priority_score
5. title
6. problem
7. persona
8. jtbd
9. context
10. monetizable
11. mvp
12. alternatives
13. willingness_to_pay_signal
14. pain_score_llm
15. pain_score_final
16. heuristic_score
17. traction_score
18. novelty_score
19. source_mix
20. example_urls

---

## Configuration Avanc√©e

### Ajuster le Nombre de Clusters

```bash
# Automatique (recommand√© pour d√©buter)
python -m need_scanner run --input "data/raw/*.json"

# Manuel (si vous savez combien de th√®mes vous voulez)
python -m need_scanner run --input "data/raw/*.json" --clusters 8
```

**R√®gle g√©n√©rale** :
- <30 posts ‚Üí 3-5 clusters
- 30-100 posts ‚Üí 5-8 clusters
- 100-200 posts ‚Üí 8-12 clusters
- 200+ posts ‚Üí 10-15 clusters

### Personnaliser les Signaux WTP

√âditer `src/need_scanner/analysis/wtp.py` pour ajouter vos patterns :

```python
WTP_PATTERNS = {
    "direct_payment": [
        r"\b(willing to pay|would pay|ready to pay)\b",
        r"\b(votre pattern personnalis√©)\b",
    ],
    # ...
}
```

### Ajuster la Formule de Priority

√âditer `src/need_scanner/analysis/priority.py` :

```python
def calculate_priority_score(...):
    # Ajuster les poids
    priority = (
        combined_pain * 0.40 +    # 40% pain (modifi√©)
        traction_score * 0.30 +   # 30% traction (modifi√©)
        novelty_score * 0.20 +
        wtp_score * 0.10
    )
```

---

## Cas d'Usage

### Cas 1 : D√©couverte Freelance

```bash
# 1. Collecter
python -m need_scanner collect-reddit-multi \
  --config-file config/reddit_subs_freelance.txt \
  --limit-per-sub 50

# 2. Pr√©visualiser
python -m need_scanner prefilter \
  --filter-lang en \
  --filter-intent \
  --detect-wtp

# 3. Analyser
python -m need_scanner run \
  --input "data/raw/posts_reddit*.json" \
  --clusters 8 \
  --output-dir data/freelance_insights
```

### Cas 2 : Veille Tech Startup

```bash
# Multi-sources
python -m need_scanner collect-all \
  --reddit-subreddit startups \
  --hn-days 7 \
  --filter-lang en

python -m need_scanner collect-stackexchange \
  --sites startups,entrepreneur

# Analyse
python -m need_scanner run \
  --input "data/raw/posts_*.json" \
  --clusters 10
```

### Cas 3 : March√© Fran√ßais

```bash
# Collecter subreddits FR
python -m need_scanner collect-reddit-multi \
  --config-file config/reddit_subs_fr.txt

# Filtrer FR uniquement
python -m need_scanner prefilter \
  --filter-lang fr \
  --detect-wtp

# Analyser
python -m need_scanner run --input "data/raw/*.json"
```

---

## Troubleshooting

### Erreur : "Not enough posts after filtering"

**Cause** : Trop de posts filtr√©s

**Solutions** :
1. Collecter plus de posts : `--limit-per-sub 50`
2. √âlargir filtres langue : `--filter-lang en,fr`
3. Inclure plus d'intents : Modifier le code pour inclure "other"

### Erreur : "OpenAI API error"

**Causes possibles** :
- API key invalide
- Quota d√©pass√©
- Rate limit

**Solutions** :
1. V√©rifier `.env` : `OPENAI_API_KEY=sk-...`
2. V√©rifier solde OpenAI
3. R√©duire charge : `--max-examples 3` ou `--clusters 3`

### Co√ªts trop √©lev√©s

**Solutions** :
1. Utiliser `estimate` avant de lancer
2. R√©duire nombre de posts en entr√©e
3. R√©duire nombre de clusters
4. R√©duire `--max-examples`

### Clusters de mauvaise qualit√©

**Causes** :
- Trop peu de posts
- Posts trop diversifi√©s
- Mauvais nombre de clusters

**Solutions** :
1. Filtrer par secteur/subreddit sp√©cifique
2. Augmenter le nombre de posts
3. Ajuster `--clusters` (essayer ¬±2)
4. Filtrer par intent plus strictement

### R√©sultats en mauvaise langue

**Solution** :
```bash
python -m need_scanner prefilter --filter-lang en
# V√©rifier distribution AVANT de lancer pipeline
```

---

## Workflows Recommand√©s

### Workflow 1 : Exploration Rapide

```bash
# 1. Collecter petit √©chantillon
python -m need_scanner collect-reddit-multi --limit-per-sub 10

# 2. Pr√©visualiser
python -m need_scanner prefilter --detect-wtp

# 3. Analyser
python -m need_scanner run --clusters 3
```

**Dur√©e** : ~5 minutes
**Co√ªt** : ~$0.001

### Workflow 2 : Analyse Compl√®te

```bash
# 1. Collecter large
python -m need_scanner collect-reddit-multi --limit-per-sub 30
python -m need_scanner collect-hn --days 30
python -m need_scanner collect-stackexchange --sites-file config/stackexchange_sites.txt

# 2. Pr√©visualiser et d√©cider filtres
python -m need_scanner prefilter --filter-lang en,fr --detect-wtp

# 3. Estimer co√ªt
python -m need_scanner estimate --clusters 10

# 4. Analyser
python -m need_scanner run --clusters 10 --output-dir data/full_analysis
```

**Dur√©e** : ~30 minutes
**Co√ªt** : ~$0.02-0.05

---

## Astuces et Bonnes Pratiques

### Collecte

‚úÖ **√Ä faire** :
- Collecter r√©guli√®rement (hebdomadaire/mensuel)
- Varier les sources pour diversit√©
- Utiliser `prefilter` avant analyse
- Sauvegarder les configs fonctionnelles

‚ùå **√Ä √©viter** :
- Collecter trop de posts d'un coup (co√ªt)
- Ignorer la distribution des langues
- Lancer pipeline sans `estimate`

### Analyse

‚úÖ **√Ä faire** :
- Commencer avec peu de clusters (3-5)
- V√©rifier les insights manuellement
- Comparer les priority scores
- Exporter en CSV pour partage

‚ùå **√Ä √©viter** :
- Trop de clusters (fragmentation)
- Ignorer les scores bas (peuvent √™tre int√©ressants)
- Se fier uniquement au LLM (v√©rifier exemples)

### Production

‚úÖ **√Ä faire** :
- Versionner les configs
- Documenter vos d√©couvertes
- Cr√©er des scripts personnalis√©s
- Monitorer les co√ªts

---

**Besoin d'aide ?** Consultez les autres docs dans `docs/` ou ouvrez une issue.
