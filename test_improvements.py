"""
Test script for engine improvements.

Tests:
1. Configuration multi-model
2. Sector classification
3. MMR reranking
4. History management
"""

import numpy as np
from pathlib import Path

# Test imports
try:
    from src.need_scanner.config import get_config
    from src.need_scanner.analysis.sector import classify_cluster_sector, SECTORS
    from src.need_scanner.processing.mmr import mmr_rerank
    from src.need_scanner.processing.history import ClusterHistory
    from src.need_scanner.fetchers.balanced_sampling import load_sources_config
    print("âœ… All imports successful")
except ImportError as e:
    print(f"âŒ Import error: {e}")
    exit(1)


def test_config():
    """Test multi-model configuration."""
    print("\n" + "=" * 60)
    print("TEST 1: Multi-Model Configuration")
    print("=" * 60)

    config = get_config()

    print(f"âœ“ Light model: {config.ns_light_model}")
    print(f"âœ“ Heavy model: {config.ns_heavy_model}")
    print(f"âœ“ TOP K enrichment: {config.ns_top_k_enrichment}")
    print(f"âœ“ MMR lambda: {config.ns_mmr_lambda}")
    print(f"âœ“ History retention: {config.ns_history_retention_days} days")
    print(f"âœ“ History penalty factor: {config.ns_history_penalty_factor}")

    assert config.ns_light_model == "gpt-4o-mini"
    assert config.ns_heavy_model == "gpt-4o"
    assert config.ns_top_k_enrichment > 0
    assert 0 <= config.ns_mmr_lambda <= 1
    assert config.ns_history_retention_days > 0

    print("\nâœ… Configuration test passed")


def test_sectors():
    """Test sector classification."""
    print("\n" + "=" * 60)
    print("TEST 2: Sector Classification")
    print("=" * 60)

    print(f"Available sectors ({len(SECTORS)}):")
    for sector in SECTORS:
        print(f"  - {sector}")

    # Test sector classification (mock - doesn't call API)
    print("\nâœ“ Sectors defined correctly")
    print("âœ… Sector test passed (API calls skipped in test mode)")


def test_mmr():
    """Test MMR reranking."""
    print("\n" + "=" * 60)
    print("TEST 3: MMR Reranking")
    print("=" * 60)

    # Mock data
    n_items = 10
    embeddings = np.random.rand(n_items, 128)
    priority_scores = np.random.rand(n_items) * 10

    items = [
        {'id': i, 'title': f'Cluster {i}'}
        for i in range(n_items)
    ]

    print(f"Mock data: {n_items} clusters")

    # MMR rerank
    reranked_items, selected_indices = mmr_rerank(
        items=items,
        embeddings=embeddings,
        priority_scores=priority_scores,
        top_k=5,
        lambda_param=0.7
    )

    print(f"\nâœ“ Selected {len(reranked_items)} items via MMR")
    print(f"âœ“ Selected indices: {selected_indices}")
    print(f"âœ“ MMR ranks assigned: {[item['mmr_rank'] for item in reranked_items]}")

    assert len(reranked_items) == 5
    assert len(selected_indices) == 5
    assert all('mmr_rank' in item for item in reranked_items)

    print("\nâœ… MMR test passed")


def test_history():
    """Test cluster history management."""
    print("\n" + "=" * 60)
    print("TEST 4: Cluster History")
    print("=" * 60)

    # Create test history
    test_history_path = Path("data/test_history_temp.jsonl")
    test_history_path.parent.mkdir(parents=True, exist_ok=True)

    history = ClusterHistory(test_history_path)

    print(f"âœ“ History initialized: {len(history.entries)} entries")

    # Add mock clusters
    mock_summaries = [
        {'cluster_id': 0, 'title': 'Test cluster 1', 'problem': 'Test problem', 'sector': 'dev_tools'},
        {'cluster_id': 1, 'title': 'Test cluster 2', 'problem': 'Test problem', 'sector': 'business_pme'}
    ]
    mock_embeddings = np.random.rand(2, 128)
    mock_scores = [7.5, 6.8]

    history.add_clusters(
        cluster_summaries=mock_summaries,
        embeddings=mock_embeddings,
        priority_scores=mock_scores,
        date="2025-01-25"
    )

    print(f"âœ“ Added {len(mock_summaries)} clusters to history")

    # Test similarity penalty
    new_embeddings = np.random.rand(3, 128)
    penalties = history.compute_similarity_penalty(new_embeddings, penalty_factor=0.3)

    print(f"âœ“ Computed penalties: {penalties}")
    assert len(penalties) == 3
    assert all(0 <= p <= 0.3 for p in penalties)

    # Cleanup
    if test_history_path.exists():
        test_history_path.unlink()
        print("âœ“ Test history cleaned up")
    else:
        print("âœ“ Test history already cleaned (auto-cleanup)")

    print("\nâœ… History test passed")


def test_sources_config():
    """Test sources configuration."""
    print("\n" + "=" * 60)
    print("TEST 5: Sources Configuration")
    print("=" * 60)

    config_path = Path("config/sources_config.yaml")

    if not config_path.exists():
        print(f"âš ï¸  Sources config not found at {config_path}")
        print("âœ… Test skipped (file will be created when needed)")
        return

    config = load_sources_config(config_path)

    print(f"âœ“ Reddit sources: {len(config.get('reddit_sources', []))}")
    print(f"âœ“ StackExchange sources: {len(config.get('stackexchange_sources', []))}")
    print(f"âœ“ Category quotas: {len(config.get('category_quotas', {}))}")

    # Show sample
    if config.get('reddit_sources'):
        sample = config['reddit_sources'][0]
        print(f"\nâœ“ Sample Reddit source:")
        print(f"  - Name: {sample.get('name')}")
        print(f"  - Category: {sample.get('category')}")
        print(f"  - Max posts: {sample.get('max_posts')}")

    print("\nâœ… Sources config test passed")


def main():
    """Run all tests."""
    print("=" * 60)
    print("ðŸ§ª NEED SCANNER - Engine Improvements Tests")
    print("=" * 60)

    try:
        test_config()
        test_sectors()
        test_mmr()
        test_history()
        test_sources_config()

        print("\n" + "=" * 60)
        print("âœ… ALL TESTS PASSED")
        print("=" * 60)
        print("\nEngine improvements are working correctly!")
        print("\nNext steps:")
        print("  1. Configure .env with OpenAI API key")
        print("  2. Run the enriched pipeline:")
        print("     python -m src.need_scanner.jobs.enriched_pipeline")
        print("  3. See docs/ENGINE_IMPROVEMENTS.md for full documentation")

    except Exception as e:
        print(f"\nâŒ Test failed: {e}")
        import traceback
        traceback.print_exc()
        exit(1)


if __name__ == "__main__":
    main()
