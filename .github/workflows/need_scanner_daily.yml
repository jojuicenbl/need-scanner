name: Need Scanner Daily

on:
  schedule:
    # Run every day at 06:15 UTC (08:15 Paris time)
    - cron: "15 6 * * *"
  # Allow manual trigger
  workflow_dispatch:
    inputs:
      pack:
        description: 'Subreddit pack to use'
        required: false
        default: 'smallbiz_fr'
      reddit_limit:
        description: 'Posts per subreddit'
        required: false
        default: '240'

jobs:
  scan:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install -e .

      - name: Create data directories
        run: |
          mkdir -p data/raw
          mkdir -p data/history
          mkdir -p data/daily

      - name: Collect posts
        env:
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
        run: |
          python -m need_scanner collect-all \
            --pack ${{ github.event.inputs.pack || 'smallbiz_fr' }} \
            --reddit-limit ${{ github.event.inputs.reddit_limit || '240' }} \
            --reddit-mode hot \
            --hn-days 30 \
            --rss-days 30 \
            --include-keywords-file config/intent_patterns.txt \
            --history-days 45 \
            --filter-lang en,fr \
            --filter-intent

      - name: Prefilter posts
        run: |
          python -m need_scanner prefilter \
            --filter-lang en,fr \
            --filter-intent \
            --keep-intents pain,request \
            --detect-wtp

      - name: Run pipeline
        env:
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
        run: |
          python -m need_scanner run \
            --clusters 12 \
            --novelty-weight 0.15 \
            --trend-weight 0.15 \
            --history-path data/history \
            --output-dir data/daily/$(date +%Y%m%d)

      - name: Upload results
        uses: actions/upload-artifact@v4
        with:
          name: daily-insights-${{ github.run_number }}
          path: |
            data/daily/**/insights_enriched.csv
            data/daily/**/cluster_results.json
          retention-days: 30

      # Optional: Commit history files back to repo
      - name: Commit history
        if: success()
        run: |
          git config --local user.email "action@github.com"
          git config --local user.name "GitHub Action"
          git add data/history/*.json || true
          git diff --quiet && git diff --staged --quiet || git commit -m "Update history [skip ci]"
          git push || true

      # Send Slack notification with Python (handles special characters properly)
      - name: Send Slack notification
        if: success()
        env:
          SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK_URL }}
        run: |
          OUTPUT_DIR="data/daily/$(date +%Y%m%d)"

          python3 << 'EOF'
          import json
          import os
          import csv
          import urllib.request

          output_dir = os.environ.get('OUTPUT_DIR', 'data/daily')
          webhook_url = os.environ['SLACK_WEBHOOK_URL']
          run_url = "${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}"

          # Extract metrics
          try:
              with open(f'{output_dir}/meta.json', 'r') as f:
                  meta = json.load(f)
                  total_posts = len(meta)
          except:
              total_posts = "N/A"

          try:
              with open(f'{output_dir}/insights_enriched.csv', 'r', encoding='utf-8') as f:
                  reader = csv.DictReader(f)
                  rows = list(reader)
                  clusters = len(rows)
                  if rows:
                      top = rows[0]
                      top_title = top['title']
                      top_score = top['priority_score']
                  else:
                      top_title = "N/A"
                      top_score = "N/A"
          except:
              clusters = "N/A"
              top_title = "N/A"
              top_score = "N/A"

          # Build Slack payload
          payload = {
              "text": "Need Scanner daily run complete!",
              "blocks": [
                  {
                      "type": "header",
                      "text": {
                          "type": "plain_text",
                          "text": "âœ… Need Scanner Daily Scan Complete"
                      }
                  },
                  {
                      "type": "section",
                      "fields": [
                          {
                              "type": "mrkdwn",
                              "text": f"*Posts Analyzed:*\n{total_posts}"
                          },
                          {
                              "type": "mrkdwn",
                              "text": f"*Clusters Found:*\n{clusters}"
                          }
                      ]
                  },
                  {
                      "type": "section",
                      "text": {
                          "type": "mrkdwn",
                          "text": f"*Top Priority:*\n{top_title} (Score: {top_score})"
                      }
                  },
                  {
                      "type": "section",
                      "text": {
                          "type": "mrkdwn",
                          "text": f"<{run_url}|View Full Results>"
                      }
                  }
              ]
          }

          # Send to Slack
          req = urllib.request.Request(
              webhook_url,
              data=json.dumps(payload).encode('utf-8'),
              headers={'Content-Type': 'application/json'}
          )

          with urllib.request.urlopen(req) as response:
              print(f"Slack notification sent: {response.status}")
          EOF

      - name: Cleanup old artifacts
        uses: geekyeggo/delete-artifact@v5
        if: success()
        with:
          name: daily-insights-*
          useGlob: true
          failOnError: false
          # Keep last 7 days
          retentionDays: 7
