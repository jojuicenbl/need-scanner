name: Need Scanner Daily

on:
  schedule:
    # Run every day at 06:15 UTC (08:15 Paris time)
    - cron: "15 6 * * *"
  # Allow manual trigger
  workflow_dispatch:
    inputs:
      pack:
        description: 'Subreddit pack to use'
        required: false
        default: 'multi_sector'
      reddit_limit:
        description: 'Posts per subreddit'
        required: false
        default: '240'

jobs:
  scan:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install -e .

      - name: Create data directories
        run: |
          mkdir -p data/raw
          mkdir -p data/history
          mkdir -p data/daily

      - name: Collect posts
        env:
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
        run: |
          python -m need_scanner collect-all \
            --pack ${{ github.event.inputs.pack || 'multi_sector' }} \
            --reddit-limit ${{ github.event.inputs.reddit_limit || '240' }} \
            --reddit-mode hot \
            --hn-days 30 \
            --rss-feeds-file config/rss_feeds.txt \
            --rss-days 30 \
            --include-keywords-file config/intent_patterns.txt \
            --history-days 45 \
            --filter-lang en,fr \
            --filter-intent

      - name: Prefilter posts
        run: |
          python -m need_scanner prefilter \
            --filter-lang en,fr \
            --filter-intent \
            --keep-intents pain,request \
            --detect-wtp

      - name: Run pipeline v2.0
        env:
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
        run: |
          python scripts/run_github_actions_v2.py

      - name: Verify output files exist
        run: |
          echo "Listing data/daily directory:"
          ls -laR data/daily/
          echo ""
          echo "Checking for CSV files:"
          find data/daily -name "*.csv" -type f || echo "No CSV files found"
          echo ""
          echo "Checking for JSON files:"
          find data/daily -name "*.json" -type f || echo "No JSON files found"

      - name: Upload results
        uses: actions/upload-artifact@v4
        with:
          name: daily-insights-${{ github.run_number }}
          path: |
            data/daily/**/insights_enriched.csv
            data/daily/**/cluster_results.json
          retention-days: 30
          if-no-files-found: error

      # Optional: Commit history files back to repo
      - name: Commit history
        if: success()
        run: |
          git config --local user.email "action@github.com"
          git config --local user.name "GitHub Action"
          git add data/history/*.json || true
          git diff --quiet && git diff --staged --quiet || git commit -m "Update history [skip ci]"
          git push || true

      # Send Slack notification with Python (handles special characters properly)
      - name: Send Slack notification
        if: success()
        env:
          SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK_URL }}
        run: |
          export OUTPUT_DIR="data/daily/$(date +%Y%m%d)"

          python3 << 'EOF'
          import json
          import os
          import csv
          import urllib.request
          import traceback

          output_dir = os.environ.get('OUTPUT_DIR')
          webhook_url = os.environ['SLACK_WEBHOOK_URL']
          run_url = "${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}"

          print(f"Looking for files in: {output_dir}")

          # Extract metrics
          try:
              meta_path = f'{output_dir}/meta.json'
              with open(meta_path, 'r') as f:
                  meta = json.load(f)
                  total_posts = len(meta)
              print(f"Found {total_posts} posts")
          except Exception as e:
              print(f"Error reading meta.json: {e}")
              total_posts = "N/A"

          # Read cluster results for cost info
          try:
              results_path = f'{output_dir}/cluster_results.json'
              with open(results_path, 'r') as f:
                  results = json.load(f)
                  cost_info = results.get('cost_breakdown', {})
                  total_cost = cost_info.get('total', 'N/A')
          except:
              total_cost = "N/A"

          # Extract top clusters (v2.0 format)
          top_clusters = []
          clusters_count = "N/A"
          sectors_stats = {}
          try:
              csv_path = f'{output_dir}/insights_enriched.csv'
              with open(csv_path, 'r', encoding='utf-8') as f:
                  reader = csv.DictReader(f)
                  rows = list(reader)
                  clusters_count = len(rows)

                  # Count sectors
                  for row in rows:
                      sector = row.get('sector', 'other')
                      sectors_stats[sector] = sectors_stats.get(sector, 0) + 1

                  # Get top 5 (or all if less than 5)
                  for i, row in enumerate(rows[:5], 1):
                      top_clusters.append({
                          'rank': i,
                          'mmr_rank': row.get('mmr_rank', 'N/A'),
                          'title': row['title'],
                          'sector': row.get('sector', 'other'),
                          'score': float(row['priority_score']),
                          'score_adjusted': float(row.get('priority_score_adjusted', row['priority_score'])),
                          'pain': row.get('pain_score_final', 'N/A'),
                          'novelty': row.get('novelty_score', 'N/A'),
                          'trend': row.get('trend_score', 'N/A'),
                          'size': row.get('size', 'N/A')
                      })
                  print(f"Found {clusters_count} clusters across {len(sectors_stats)} sectors")
          except Exception as e:
              print(f"Error reading insights: {e}")
              traceback.print_exc()

          # Build Slack blocks
          blocks = [
              {
                  "type": "header",
                  "text": {
                      "type": "plain_text",
                      "text": "ðŸŽ¯ Need Scanner Daily Results"
                  }
              },
              {
                  "type": "section",
                  "fields": [
                      {
                          "type": "mrkdwn",
                          "text": f"*ðŸ“Š Posts Analyzed*\n{total_posts}"
                      },
                      {
                          "type": "mrkdwn",
                          "text": f"*ðŸŽª Clusters Found*\n{clusters_count}"
                      },
                      {
                          "type": "mrkdwn",
                          "text": f"*ðŸ’° Total Cost*\n${total_cost}"
                      },
                      {
                          "type": "mrkdwn",
                          "text": f"*ðŸ“… Date*\n{os.path.basename(output_dir)}"
                      }
                  ]
              },
              {"type": "divider"}
          ]

          # Add sector diversity info (v2.0 feature)
          if sectors_stats:
              sectors_text = " | ".join([f"{sector}: {count}" for sector, count in sorted(sectors_stats.items())[:5]])
              blocks.append({
                  "type": "section",
                  "text": {
                      "type": "mrkdwn",
                      "text": f"*ðŸŽ¨ Sector Diversity (v2.0)*\n{sectors_text}"
                  }
              })
              blocks.append({"type": "divider"})

          # Add top 5 priorities (v2.0 format)
          if top_clusters:
              blocks.append({
                  "type": "section",
                  "text": {
                      "type": "mrkdwn",
                      "text": "*ðŸ† Top 5 Priorities (MMR Ranked)*"
                  }
              })

              # Sector emojis
              sector_emojis = {
                  'dev_tools': 'ðŸ’»',
                  'ai_llm': 'ðŸ¤–',
                  'business_pme': 'ðŸ’¼',
                  'health_wellbeing': 'ðŸ¥',
                  'education_learning': 'ðŸ“š',
                  'ecommerce_retail': 'ðŸ›’',
                  'marketing_sales': 'ðŸ“Š',
                  'creator_economy': 'ðŸŽ¨',
                  'workplace_hr': 'ðŸ‘”',
                  'finance_accounting': 'ðŸ’°',
                  'legal_compliance': 'âš–ï¸',
                  'consumer_lifestyle': 'ðŸ ',
                  'other': 'ðŸ“Œ'
              }

              for cluster in top_clusters:
                  emoji = "ðŸ¥‡" if cluster['rank'] == 1 else "ðŸ¥ˆ" if cluster['rank'] == 2 else "ðŸ¥‰" if cluster['rank'] == 3 else "â­"
                  sector_emoji = sector_emojis.get(cluster['sector'], 'ðŸ“Œ')

                  blocks.append({
                      "type": "section",
                      "text": {
                          "type": "mrkdwn",
                          "text": f"{emoji} *#{cluster['rank']} {sector_emoji} [{cluster['sector']}] - {cluster['title']}*\n"
                                  f"Priority: `{cluster['score']:.2f}` â†’ `{cluster['score_adjusted']:.2f}` (adjusted) | "
                                  f"MMR: `#{cluster['mmr_rank']}` | "
                                  f"Pain: `{cluster['pain']}` | "
                                  f"Novelty: `{cluster['novelty']}` | "
                                  f"Trend: `{cluster['trend']}` | "
                                  f"Size: `{cluster['size']}`"
                      }
                  })

              blocks.append({"type": "divider"})

          # Add footer with instructions
          blocks.append({
              "type": "section",
              "text": {
                  "type": "mrkdwn",
                  "text": "ðŸ’¡ *How to download full results:*\n"
                          "Click the button below â†’ Scroll to *Artifacts* section â†’ Download `daily-insights-XXX.zip`\n\n"
                          "_âœ¨ Powered by Need Scanner v2.0 - Multi-sector, MMR ranking, history-based deduplication_"
              }
          })

          # Add action button
          blocks.append({
              "type": "actions",
              "elements": [
                  {
                      "type": "button",
                      "text": {
                          "type": "plain_text",
                          "text": "ðŸ“Š View Results & Download CSV"
                      },
                      "url": run_url,
                      "style": "primary"
                  }
              ]
          })

          # Build payload
          payload = {
              "text": "Need Scanner daily run complete!",
              "blocks": blocks
          }

          # Send to Slack
          req = urllib.request.Request(
              webhook_url,
              data=json.dumps(payload).encode('utf-8'),
              headers={'Content-Type': 'application/json'}
          )

          with urllib.request.urlopen(req) as response:
              print(f"Slack notification sent: {response.status}")
          EOF

      # - name: Cleanup old artifacts
      #   uses: geekyeggo/delete-artifact@v5
      #   if: success()
      #   with:
      #     name: daily-insights-*
      #     useGlob: true
      #     failOnError: false
